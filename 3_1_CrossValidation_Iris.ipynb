{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ5mkLCje1uhA4VO4LQX4K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation\n",
        "Exemplo de Validação Cruzada, utilizando o dataset Iris por ser um dataset pequeno com apenas 150 elementos. Isso facilita a explicação sobre as K-Pastas que o processo separa"
      ],
      "metadata": {
        "id": "ZnTKw8ivz-Uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n8Q1GVYBuyAS"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X_train = iris.data\n",
        "y_train = iris.target"
      ],
      "metadata": {
        "id": "ToAAruotu8E5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ideia por trás do processo de Validação Cruzada é permitir que o conjunto de dados utilizado no treinamento do modelo não seja tão diferente do conjunto de dados utilizado para testar. Em casos de utilização do Hold-out (seja com separação 70/30, 80/20 ou qualquer outra) pode acontecer de uma determinada classe da variável alvo ficar de fora ou ter baixa representavidade durante o treino. Ao executar o processo com validação cruzada, todo o conjunto de dados será utilizado para treino e para teste.\n",
        "\n",
        "O processo busca separar o conjunto de dados em K-Pastas, dividindo todos os dados do dataset em partes iguais e atribuindo estas partes às pastas. Neste exemplo do Iris, que possui 150 observações (linhas de dados), vamos fazer a separação em 10 pastas (preste atenção à estes numeros para entender o que será feito).\n",
        "\n",
        "Se dividirmos 150 / 10 (150 linhas por 10 pastas) teremos o valor de 15. Este será o total de elementos que o processo de Cross Validation irá separar em cada pasta para que os dados sejam testados, e todos os outros valores do dataset serão utilizados para treinar o modelo.\n",
        "\n",
        "Ao fazer isso 10 vezes (que é o total de pastas que foi definido neste exemplo), o processo de Validação Cruzada conseguirá treinar o modelo com todos os dados e também testar o modelo com todos os dados.\n",
        "\n",
        "Porém, para garantir esse teste com todas as observações do dataset, o Cross Validation garante que não haja repetição nos dados de teste em nenhuma das pastas, ou seja, os dados que foram utilizados para testar o algoritmo na pasta 1 será diferente da pasta 2, 3, ... 10. \n",
        "\n",
        "\n",
        "Agora repare o código abaixo, onde é chamada a função $StratifiedKFold$ e são passados alguns parâmetros. O primeiro $n_splits$ define a quantidade de pastas que o dataset será desmembrado. Neste caso do exemplo é 10. O $random_state$ define qual é a semente de pseudo-aleatoriedade para garantir uma reproducibilidade do experimento, e por fim o $shuffle$ irá embaralhar os dados para este conjunto de dados. "
      ],
      "metadata": {
        "id": "z2nTOHfR0q-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfolds = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "JxRV3L97vZZP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após criar o objeto $kfolds$ é hora de separar os elementos do dataset a partir destas configurações. Acompanhe o código abaixo e veja que não há repetição em nenhum dos dados de $y$ que foram separados pelo método. Veja que este objeto possui 10 linhas e 15 colunas, sendo que cada linha representa uma pasta e cada coluna é o índice que o processo irá utilizar para fazer o $lookup$ do dado no dataset original"
      ],
      "metadata": {
        "id": "6f421m746bNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for treino, teste in kfolds.split(X_train, y_train):\n",
        "  print(teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLP-HoUy6Uuj",
        "outputId": "ceb5b8be-c6ce-4645-e652-803e811d81bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 11  14  25  33  35  52  59  65  87  97 100 107 124 130 144]\n",
            "[ 13  15  20  29  45  78  85  92  94  99 102 112 138 139 148]\n",
            "[  0  10  31  40  47  58  68  74  80  90 111 114 120 121 136]\n",
            "[  4   9  19  21  42  60  75  82  89  96 106 108 109 110 149]\n",
            "[ 22  34  38  41  44  56  64  66  70  71 116 118 127 132 133]\n",
            "[  6   7  27  30  48  50  72  83  95  98 104 105 123 126 140]\n",
            "[  2   8  23  24  28  61  63  67  69  91 122 125 135 145 147]\n",
            "[  1  12  32  37  49  51  55  79  84  88 115 117 128 129 142]\n",
            "[ 16  26  36  39  46  57  73  76  77  93 103 113 131 141 143]\n",
            "[  3   5  17  18  43  53  54  62  81  86 101 119 134 137 146]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Repare que implementei um laço de repetição para percorrer os índices que foram gerados neste objeto, armazenando os elementos do dataset em uma lista para que possa ser utilizada posteriormente (no momento de treinar modelos com estes dados)."
      ],
      "metadata": {
        "id": "yFK9BOna24bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_treino, y_treino, X_teste, y_teste = [],[],[],[]\n",
        "\n",
        "for idx_treino, idx_teste in kfolds.split(X_train, y_train):\n",
        "  X_treino.append(X_train[idx_treino])\n",
        "  y_treino.append(y_train[idx_treino])\n",
        "  X_teste.append(X_train[idx_teste])\n",
        "  y_teste.append(y_train[idx_teste])"
      ],
      "metadata": {
        "id": "Qj-XR0Hiv8dd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após essa separação, é possível imprimir os elementos para verificar como ficaram as pastas. Depois disso é só utilizar estes elementos para treinar os seus modelos, mantendo o mesmo conjunto de dados para treino e para teste. Assim será possível comparar as medidas de avaliação de performance dos modelos de forma justa, pois foram treinados e testados exatamente com os mesmos dados."
      ],
      "metadata": {
        "id": "EQBw5eTs3ZCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_treino[0]))\n",
        "print(len(y_treino[0]))\n",
        "print(len(X_teste[0]))\n",
        "print(len(y_teste[0]))\n",
        "\n",
        "print(len(X_treino[0][0]))\n",
        "print(len(X_teste[0][0]))\n",
        "print(y_treino[0])\n",
        "print(y_teste[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WQoxr8ux1Hn",
        "outputId": "12bda264-c11e-451f-fd65-d4348c6fd8e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135\n",
            "135\n",
            "15\n",
            "15\n",
            "4\n",
            "4\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnedkAFH7l7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}